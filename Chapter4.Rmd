# Chapter 4: Clustering and classification

<!--  
*Describe the work you have done this week and summarize your learning.*

- Describe your work and results clearly. 
- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods
- Assume the reader has no previous knowledge of your data or the more advanced methods you are using  -->

Again starting off by loading all the packages needed for this analysis
```{r}
# access the MASS package and load the data
library(MASS)
library(dplyr)
library(ggplot2)
library(GGally)
data("Boston")
```

## Data

The dataset used in these analysis describes housing values in the suburbs of Boston. It is freely availble in the MASS package ([more information here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html)). 

The dataset has 506 observations and 14 columns. 

```{r}
# explore data dimension and structures
dim(Boston)
str(Boston)
summary(Boston)
```

Next, let's use pairs to get a graphical overview of the data. It seems that there are some variables seem to correlate positively or negatively, however not necessarily linear. In this analysis we are interested in the crime rates in particular. In the graph it seems that certain variables are heavile clustered based on low or high crime rates, such as chad (Charles River dummy variable), rad (index of accessibility to radial highways), tax rate and the proportion of blacks by town. However, some statistical analysis is required to show if this perception is actually true.

```{r}
# graphical overview of the data
pairs(Boston)
```
<!--  
# just a personal memo here
#library(tidyverse, MASS, corrplot)
# calculate the correlation matrix and round it
#cor_matrix<-cor(Boston) %>% round(digits=2)

# print the correlation matrix
#cor_matrix 

# visualize the correlation matrix
#corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos ="d", tl.cex=0.6) -->

Next, let's standardize the dataset for the LDA analysis. Scaling subtracts the column means from the corresponding columns and divides the difference with standard deviation. We end up having variables that all have a mean of zero.

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled) #otherwise its matrix
```
## Linear Discriminant Analysis LDA

Next, we generate a new categorial variable of the crime variable (crim) by using the quantiles (quartiles) as break points. This new variable replaces the continuous crime variable in the dataset.

```{r}
# save the scaled crim as scaled_crim
scaled_crim <- boston_scaled$crim
# create a quantile vector of crim and print it
bins <- quantile(scaled_crim)
# create a categorical variable 'crime'
crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, label=c("low", "med_low", "med_high", "high"))
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Next, for the purposes of the following analysis, the dataset is divided to training and testing sets, so that 80% of the data belongs to the training set.

```{r}
n <- nrow(boston_scaled) # gives number of rows
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create training and testing sets
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
``` 

Next, we fit a linear discriminant analysis model on the training dataset, trying to predict the categorical crime variable using all other variables as predictors.

```{r}
lda.fit <- lda(crime ~ ., data = train)
lda.fit

# the function for lda biplot arrows
# original? http://stackoverflow.com/questions/17232251/how-can-i-plot-a-biplot-for-lda-in-r
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
# The argument dimen to choose how many discriminants are used. 
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

In this model, LD1 explains 94,8 percent of the variance in crime categories, while LD2 and LD3 have small effects only. If I am interpreting the plot correctly, the LDA biplot shows that the category of high crime rate town areas are separated from other areas by their index of accessibility to radial highways (variable rad).

```{r}
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

When compared to the testing data set, in this LDA model all high crime rate areas are correctly classified. 11 medium high crime rate areas are falsely classified as medium low and 15 low crime areas to medium low. Hence, this model predicts well the high crime rate areas but some inaccuracies exist in other categories. Apart from low, majority of observations fall into correct categories.

## K-means

Next let's try another statistical method with the dataset, k-means: an unsupervised clustering method that assigns observations to groups or clusters based on similarity. In order to do this, we need to reload and standardize the Boston dataset to get comparable data. We calculate the distances using Eucleadian distance measure from standard R tools. 

```{r}
# reload Boston and scale it
data('Boston')
boston_scaledk <- scale(Boston)
boston_scaledk <- as.data.frame(boston_scaledk) #otherwise matrix
# calculate euclidean distance matrix
dist_eu <- dist(boston_scaledk) # obs dist takes time with large datasets
summary(dist_eu)
```

Next, k-means function is used to run clustering. 

```{r}
# k-means clustering
km <-kmeans(dist_eu, centers = 5)

# plot the Boston dataset with clusters
pairs(boston_scaledk, col = km$cluster)
```

Total of within cluster sum of squares (WCSS) is used to determine the optimal number of clusters. We use *set.seed* to make sure same random numbers are generated in all laps and hence to ensure all results are reproducible. 

```{r}
set.seed(123) # to make sure different k's are comparable
k_max <- 10 # determine the max number of clusters

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})

# visualize the results
plot(1:k_max, twcss, type='b')
```

WCSS plot shows that the optimal number of clusters in this dataset is 2: that is where the total sum drops clearly. Therefore we run the final k-means clustering with 2 centers and visualize it.

```{r}
# k-means clustering
km <-kmeans(dist_eu, centers = 2)

# plot the Boston dataset with clusters that didn't work out
# ggpairs(boston_scaledk, mapping = aes(col = cluster))

# plot the Boston dataset with clusters
pairs(boston_scaledk, col = km$cluster)

```


## Conclusion

Both analysis point out to the fact that different suburbs of Boston are differentiated to two groups based on the crime rates: high crime rates separate from others. LDA implies this difference is most clearly visible with the variable that measures ditance from radius highways. Also variables such as full-value property-tax rate and proportion of residential land zoned for lots over 25,000 sq.ft seem to matter. It would be interesting to investigate the effect of highways more closely and to find out for instance if different traffic violations are included in the data and that distorts the observations.

### References:

(none)
```{r}

```
